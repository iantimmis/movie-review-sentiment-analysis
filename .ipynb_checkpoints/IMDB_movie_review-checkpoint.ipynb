{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis on IMDB movie reviews using GloVe word embeddings and deep LSTM network\n",
    "\n",
    "This is a draft only showing the ability to convert an example of an IMDB movie review into a vectorized representation using a 50-dimensional GloVe word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Dropout, Input, Activation, Embedding, Dense\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "# Linear Algebra\n",
    "import numpy as np\n",
    "\n",
    "# Regex\n",
    "import re\n",
    "\n",
    "# File manipulation\n",
    "import glob, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the IMDB dataset\n",
    "Dataset can be downloaded here: http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    '''\n",
    "    Loads the training and testing examples.\n",
    "    '''\n",
    "    \n",
    "    # Load the training set\n",
    "    \n",
    "    pos_train = glob.glob(\"aclImdb_v1/aclImdb/train/pos/*.txt\")\n",
    "    neg_train = glob.glob(\"aclImdb_v1/aclImdb/train/neg/*.txt\")\n",
    "    \n",
    "    X_train = sorted(pos_train + neg_train)\n",
    "\n",
    "    # Load the testing set\n",
    "    \n",
    "    pos_test = glob.glob(\"aclImdb_v1/aclImdb/test/pos/*.txt\")\n",
    "    neg_test = glob.glob(\"aclImdb_v1/aclImdb/test/neg/*.txt\")\n",
    "\n",
    "    X_test = sorted(pos_test + neg_test)\n",
    "    \n",
    "    # Generate the labels\n",
    "    \n",
    "    Y_train = []\n",
    "    Y_test = []\n",
    "    \n",
    "    for file in X_train:\n",
    "        Y_train.append(file.split(\"\\\\\")[1].split(\"_\")[1].split('.')[0])\n",
    "        \n",
    "    for file in X_test:\n",
    "        Y_test.append(file.split(\"\\\\\")[1].split(\"_\")[1].split('.')[0])   \n",
    "        \n",
    "    return (X_train, Y_train), (X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load 50 dimensional GloVe word embeddings\n",
    "\n",
    "GloVe word embeddings can be downloaded here: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    '''\n",
    "    Imports the GloVe embedding matrix from an external file and creates several interfaces to that data\n",
    "    '''\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        word_to_index = {}\n",
    "        index_to_word = {}\n",
    "        for w in sorted(words):\n",
    "            word_to_index[w] = i\n",
    "            index_to_word[i] = w\n",
    "            i = i + 1\n",
    "    return word_to_index, index_to_word, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = \"glove.6B.50d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(glove_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding \n",
      " [ 0.62345   0.032983  0.43996   0.44996   0.85634   0.10575   0.9867\n",
      " -1.1748    0.28233   0.11164   0.14791  -0.33504  -0.54567  -0.48938\n",
      " -0.30864   0.0542    0.51353   0.25094   0.90265  -0.44953  -0.19574\n",
      " -0.059456 -0.23541   0.47732   0.14565   0.71205   0.10384   0.38435\n",
      "  0.28728  -0.62065   0.19764  -0.92376  -0.45941  -0.35899  -0.36896\n",
      " -0.022755  0.036052 -0.037406 -0.6725    0.96637   1.3847   -0.22727\n",
      " -0.21122   0.47012  -0.37961  -1.0339    0.93388   0.60006  -0.36329\n",
      " -0.078399]\n"
     ]
    }
   ],
   "source": [
    "word = \"embedding\"\n",
    "try:\n",
    "    print(word,'\\n', word_to_vec_map[word])\n",
    "except:\n",
    "    print(word, \" not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    '''\n",
    "    Removes all html tags from input text\n",
    "    '''\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', text)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punctuation(text):\n",
    "    '''\n",
    "    Adds spaces around all punctation to ensure that they get tokenized properly \n",
    "    by the .split() function later on\n",
    "    '''\n",
    "    return text.replace(\"'\", \"\").replace(\",\",\" , \") \\\n",
    "    .replace(\".\",\" . \").replace(\"!\", \" ! \").replace(\"?\", \" ? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_word_known(word):\n",
    "    '''\n",
    "    Determines whether the input word relates to a valid GloVe word embedding\n",
    "    '''\n",
    "    try:\n",
    "        _ = word_to_vec_map[word]\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unknown_words(text):\n",
    "    '''\n",
    "    Removes all words from the text not related to a valid GloVe word embedding\n",
    "    '''\n",
    "    return [word for word in text if is_word_known(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_example(review):\n",
    "    '''\n",
    "    Input: A movie review\n",
    "    Output: A clean tokenized list of words and punctuation extracted from the movie review. \n",
    "    '''\n",
    "    return remove_unknown_words(separate_punctuation(remove_html(review)).lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Longest Review\n",
    "\n",
    "We do this so we know how much to pad our other examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_review(X_train, X_test):\n",
    "    '''\n",
    "    Returns the amount of tokens in the longest review. (Tokens: words and punctation)\n",
    "    '''\n",
    "    max_len = 0\n",
    "    for file in X_train + X_test:\n",
    "        with open(file, 'r', encoding=\"utf8\") as file:\n",
    "            max_len = max(maxLen, len(clean_example(file.read())))\n",
    "\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = find_longest_review(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2627\n"
     ]
    }
   ],
   "source": [
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    Returns a Keras embedding layer populated with 50-dimensional GloVe word embeddings\n",
    "    '''\n",
    "    # Define dimensions of embedding matrix\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    emb_dim = word_to_vec_map[\"embedding\"].shape[0]\n",
    "    \n",
    "    # Initialize empty matrix\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Populate embedding matrix\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Build Embedding layer\n",
    "    embedding_layer = Embedding(input_dim=vocab_len, output_dim=emb_dim, trainable=False)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_network(input_shape, word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    Returns keras model of the neural network\n",
    "    '''\n",
    "    # Input Layer\n",
    "    sentence_indices = Input(shape=input_shape, dtype=\"int32\")\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = glove_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    # LSTM layers\n",
    "    X = LSTM(256, return_sequences=True)(embeddings)\n",
    "    X = Dropout(rate=0.5)(X)\n",
    "    X = LSTM(256, return_sequences=False)(X)\n",
    "    X = Dropout(rate=0.5)(X)\n",
    "    \n",
    "    # Output layer\n",
    "    X = Dense(1)(X)\n",
    "    \n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 2627)              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 2627, 50)          20000050  \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 2627, 256)         314368    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2627, 256)         0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 20,839,987\n",
      "Trainable params: 839,937\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = lstm_network((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Mini-Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    '''\n",
    "    Input: A list of reviews containing the complete text of a movie review\n",
    "    Output: A list of reviews containing the indices to the GloVe embedding \n",
    "        matrix for each word in the original review\n",
    "    '''\n",
    "    m = X.shape[0]\n",
    "\n",
    "    X_indices = np.zeros([m, max_len])\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        j = 0\n",
    "        sentence_words = clean_example(X[i])\n",
    "\n",
    "        for w in sentence_words:\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            j = j + 1\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(filenames, labels, batch_size, word_to_index, max_len):\n",
    "    '''\n",
    "    Generates next mini-batch of data each time this function is called\n",
    "    '''\n",
    "    file_count = len(filenames)\n",
    "    batch_count = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # Generate mini-batch features\n",
    "        batch_features = []\n",
    "        for file in filenames[beg:end]:\n",
    "            with open(file, 'r', encoding=\"utf8\") as f:\n",
    "                batch_features.append(f.read())\n",
    "        \n",
    "        batch_features = sentences_to_indices(np.array(batch_features), word_to_index, max_len)\n",
    "        \n",
    "        # Generate mini-batch labels\n",
    "        beg = batch_count * batch_size\n",
    "        end = beg + batch_size\n",
    "        batch_labels = np.array(labels[beg:end])\n",
    "        \n",
    "        # Prepare for next batch\n",
    "        batch_count = batch_count + 1\n",
    "        \n",
    "        yield batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "m = len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 68/781 [=>............................] - ETA: 56:02:12 - loss: 1.4622 - mean_absolute_error: 1.0692 - acc: 0.1820"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator(X_train, Y_train, batch_size, word_to_index, max_len), \\\n",
    "                    steps_per_epoch=m/batch_size, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
